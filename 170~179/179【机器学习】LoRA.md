**LoRA (Low-Rank Adaptation)**：一种模型微调方法，通过在预训练的深度学习模型中添加低秩矩阵，以实现更高效的参数更新和模型自适应。

<!--more-->

> 本篇文章为论文 [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685) 的笔记，图片截取自原论文。

# 1 背景

在大语言模型的应用中，往往先训练一个通用的预训练模型，再基于预训练模型进行垂直场景的微调。但大语言模型往往具有很高的参数量，例如  GPT-3 有 1750 亿的参数，如果对它做每个场景的微调，每个微调都要对所有参数进行优化，每个微调也得保存对应的微调参数，这样的训练计算量和储存需求量是不可接受的。

在这个背景下，一些优化方案被提出。例如向原模型中插入适配器层 (Adapter Layer)，微调时仅更新适配器层的参数，保持预训练参数不变。这样每个微调只需要优化极少的适配器参数，每个微调也只需要保存适配器参数，降低了训练计算量和储存需求。但向原模型中插入适配器层，会导致模型变深，在 Batch Size 较小的推理场景下，会严重拖慢模型的推理速度。

LoRA 提出了一种新式的微调方案，通过添加低秩的适配器矩阵与原参数相加实现微调效果，低秩矩阵保证了训练计算量和储存需求量较低。同时简单的相加操作，使微调参数可以和预训练参数合并，合并后的模型与原模型结果完全一致，不影响推理速度。

# 2 方法

## 2.1 总览

对于全量微调方案，公式表示如下：
$$
\max_{\Phi}\sum_{(x,y)\in Z}\sum_{t=1}^{|y|}\log(P_{\Phi}(y_t|x,y_{<t}))
$$
其中，$\Phi$ 代表预训练模型的参数，通过该公式可以看出我们要优化整个 $\Phi$ 来最大化模型的预测准确率。

对于 LoRA 微调方案，公式表示如下：
$$
\max_{\Theta}\sum_{(x,y)\in Z}\sum_{t=1}^{|y|}\log(P_{\Phi_0+\Delta\Phi(\Theta)}(y_t|x,y_{<t}))
$$
其中，$\Theta$ 代表适配器的参数，通过该公式可以看出，通过适配器参数决定了 $\Delta\Phi$ 参数，将其与预训练参数相加后则为模型的实际参数。我们要优化的适配器参数 $\Theta$ 来最大化模型的预测准确率。

## 2.2 细节

首先要注意 LoRA 中的 Low-Rank 含义，论文做出了一个重要的假设：下游微调任务对预训练参数 $W_0$ 做出的更新矩阵 $\Delta W$ 是低秩的。
$$
h=(W_0+\Delta W)x
$$
在这个假设下，就可以对 $d\times d$ 的更新矩阵进行低秩分解，将其分解为 $d\times r$ 和 $r\times d$ 的两个投影矩阵 $B$ 和 $A$。其中 $r$ 可以取得很小，这样就能显著降低更新矩阵的参数量，即从 $d^2$ 降低到 $2dr$.
$$
\begin{align}
&\begin{cases}
h=(W_0+\Delta W)x\\
\Delta W_{d\times d}=B_{d\times r}A_{r\times d}
\end{cases}\\
\Rightarrow &h=(W_0+BA)x
\end{align}
$$
上述流程可用以下图示表示：

<img src="https://assets.zouht.com/img/note/179-01.webp" style="zoom: 67%;" />

然后需要注意 $B,A$ 的初始值，其中 $A$ 被初始化为均值 $0$ 方差 $\sigma^2$ 的高斯分布，$B$ 被初始化为零矩阵，因此初始情况下 $\Delta W=BA=\boldsymbol{0}$.

最后，在 LoRA 应用于 Transformer 架构的 LLM 中时，论文提出的方法只向注意力参数添加适配层，即仅微调注意力参数 $Q,K,V,O$，而注意力层之间的多层感知机模块则不进行微调。

# 3 效果

根据论文对 GPT-3 1750 亿模型的测试，若进行全量微调，则需要 1.2TB 显存，350GB 模型大小，使用 LoRA 技术后仅需要 350GB 显存，微调模型大小 35M，同时微调速度提升了 25%. 可见 LoRA 的对降低计算和储存需求的效果显著。

那么使用 LoRA 技术后，微调效果是否有降低？根据论文实验，LoRA 微调相较于全量微调几乎没有劣化，甚至在某些数据集上优于全量微调。可见论文做出的低秩假设符合实际，降低参数量并不会显著损失效果。