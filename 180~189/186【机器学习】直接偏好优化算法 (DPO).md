**直接偏好优化算法** (Direct Preference optimization, DPO)：大语言模型强化学习中的一种偏好优化方法，其相比 PPO 更简单、稳定与高效。

<!--more-->

论文：https://arxiv.org/abs/2305.18290

# 1 传统 HFRL

对于传统的 HFRL，它的策略模型（即 LLM）和奖励模型是分离的。首先要使用偏好数据训练得到奖励模型，再借助奖励模型，通过强化学习来对模型进偏好优化。

首先需要对偏好进行建模。通常偏好数据集是二元对的形式，是一个输出相对另一个输出的胜负，这并不能表示成一个数学模型，我们希望通过绝对的数值来代表每个输出的“实力”。

Bradley-Terry 模型就可以对这种二元比较进行建模，表达人类偏好的潜奖励模型记为 $r^*(x,y)$，代表 $x$ 输入条件下输出 $y$ 的奖励值，那么 $y_1$ 相对 $y_2$ 胜的几率是：
$$
p^*(y_1\succ y_2\mid x)=\frac{e^{r^*(x,y_1)}}{e^{r^*(x,y_1)}+e^{r^*(x,y_2)}}
$$
人类标注的偏好数据就相当于在 $r^*(x,y)$ 这个潜奖励模型的采样，假如数据集是 $\mathcal{D}=\{x^{(i)},y^{(i)}_w,y^{(i)}_l\}_{i=1}^N$，那么优化的目标便是：
$$
\begin{align}
\mathcal{L}(r_\phi,\mathcal{D})&=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\frac{e^{r_\phi(x,y_w)}}{e^{r_\phi(x,y_w)}+e^{r_\phi(x,y_l)}}\right]\\
&=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\sigma(r_\phi(x,y_w)-r_\phi(x,y_l))\right]
\end{align}
$$

> OpenAI 的论文 [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) 中实际上是四选一，而不是二选一。不过差别也不大，就是分母变成四项相加。

通过训练获得奖励模型 $r_\phi$ 后，就可以开始正式的强化学习了。优化的目标是：
$$
\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y\mid x)}[r_{\phi}(x,y)]-\beta\;\mathrm{KL}[\pi_{\theta}(y\mid x),\pi_{\mathrm{ref}}(y\mid x)]
$$
使用 PPO 方法进行优化。

可以看到，传统的 HFRL 流程复杂，需要先训练奖励模型，再正式训练语言模型。DPO 针对这个问题，提出了一种将奖励模型隐含在语言模型中的方法，相当于策略模型既是语言模型，也是奖励模型。

# 2 DPO 方法

根据我对论文公式推导的理解，DPO 就是通过一些巧妙的构造，消去了一些不好计算的部分，同时将奖励值隐含在语言模型中。

首先，DPO 从传统 HFRL 的优化目标开始，对它进行变形和构造：
$$
\begin{align}
&\max_{\pi}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi(y\mid x)}[r(x,y)]-\beta\;\mathrm{KL}[\pi(y\mid x),\pi_{\mathrm{ref}}(y\mid x)]\\
\Rightarrow&\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y\mid x)}\left[r(x,y)-\beta\log\frac{\pi(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right]\\
\overset{\times-1/\beta}\Rightarrow&\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y\mid x)}\left[\log\frac{\pi(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}-\frac{1}{\beta}r(x,y)\right]\\
\Rightarrow&\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y\mid x)}\left[\log\frac{\pi(y\mid x)}{\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)}-\log Z(x)\right]\\
&\text{其中,}\;Z(x)=\sum_{y}\pi_{\mathrm{ref}}(y\mid x)\exp\left({\frac{1}{\beta}r(x,y)}\right)
\end{align}
$$
上面的变形和构造虽说看不出来有什么用，但实际上是对下面的操作做准备。

接下来，把 $\log$ 里分母的那一坨式子记为：
$$
\pi^*(y\mid x)=\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)
$$
把 $\pi^*(y\mid x)$ 回代到原来的式子：
$$
\begin{align}
&\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y\mid x)}\left[\log\frac{\pi(y\mid x)}{\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)}-\log Z(x)\right]\\
\Rightarrow&\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y\mid x)}\left[\log\frac{\pi(y\mid x)}{\pi^*(y\mid x)}-\log Z(x)\right]\\
\Rightarrow&\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y\mid x)}\left[\mathrm{KL}[\pi(y\mid x),\pi_{\mathrm{ref}}(y\mid x)]-\log Z(x)\right]
\end{align}
$$
注意到，$Z(x)$ 是只关于 $x$ 的函数，$\pi^*(y\mid x)$ 完全**与 $\pi$ 无关**。那么由于 KL 散度的最小值是 0，当且仅当两分布完全相同。那么这就代表该优化目标的最优点就是 $\pi=\pi^*$.

因此，通过上面一通变形和优化，我们直接得到了优化目标的结果：
$$
\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)
$$
但是该答案中 $Z(x)$ 的计算是很昂贵的，因此到这的结果使用价值不高，需要继续操作。

对式子进行变形：
$$
\begin{align}
\pi_{r}(y\mid x)&=\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)\\
\Rightarrow\log\pi_{r}(y\mid x)&=-\log Z(x)+\log\pi_{\mathrm{ref}}(y\mid x)+\frac{1}{\beta}r(x,y)\\
\Rightarrow r(x,y)&=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}+\beta\log Z(x)
\end{align}\
$$
那么，我们就可以把我们要建模的潜奖励模型 $r^*(x,y)$ 也用上面的方式进行变换，回代到 Bradley-Terry 模型中，然后惊奇地发现难算的 $Z(X)$ 消掉了：
$$
\begin{align}
p^*(y_1\succ y_2\mid x)&=\frac{e^{r^*(x,y_1)}}{e^{r^*(x,y_1)}+e^{r^*(x,y_2)}}\\
&=\frac{1}{1+e^{r^*(x,y_2)-r^*(x,y_1)}}\\
&=\frac{1}{1+\exp\left(\beta\log\frac{\pi^*(y_2\mid x)}{\pi_{\mathrm{ref}}(y_2\mid x)}-\beta\log\frac{\pi^*(y_1\mid x)}{\pi_{\mathrm{ref}}(y_1\mid x)}\right)}\\
\end{align}
$$
建模好奖励后，接下来就可以写出目标函数了：
$$
\mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_\mathrm{ref})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\sigma(\beta\log\frac{\pi_{\theta}(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\beta\log\frac{\pi_{\theta}(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)})\right]
$$
综上，DPO 就实现了将奖励模型隐含在语言模型，直接来进行偏好优化。