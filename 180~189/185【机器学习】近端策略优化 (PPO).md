**近端策略优化** (Proximal policy optimization, PPO)：强化学习中的一种策略优化方法，其相比 TRPO 更简单、稳定与高效。

<!--more-->

- 论文：[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- 课程：[Hung-yi Lee - DRL Lecture 2: Proximal Policy Optimization (PPO)](https://youtu.be/OAKAZhFmYoI)

# 1 重要性采样的局限

上一篇文章[策略梯度算法](https://io.zouht.com/184.html)尾部提到了从 On-Policy 转换到 Off-Policy 的方法，从数学上看起来这个转换是无损的，但在实际情况中它是有代价的。

例如对于以下例子，蓝色为 $p(x)$，红色为 $q(x)$，绿色是 $f(x)$.

<img src="https://assets.zouht.com/img/note/185-01.webp" style="zoom: 33%;" />

显然，$\mathbb{E}_{x\sim p}[f(x)]<0$，那根据重要性采样：
$$
\mathbb{E}_{x\sim p}[f(x)]=\mathbb{E}_{x\sim q}[\frac{p(x)}{q(x)}f(x)]<0
$$
这在数学上当然是成立的，但实际中需要考虑的是采样数量是有限的。

很有可能我们在 $q$ 分布中采样得到的 $x$ 全部都是 $>0$ 的，这就会导致算出 $\mathbb{E}_{x\sim p}[f(x)]>0$ 的错误结果。只有采样数量足够多（或者运气够好），采到 $x<0$ 的情况，才有可能得到更加正确的结果。

这便告诉我们，即使重要性采样在数学中是完全正确的，但在实际中需要保证 $p,q$ 两分布的差异不要太大，过大的差距会导致采样得到的期望有很大的偏差。

# 2 添加约束

首先来看重要性采样的策略梯度算法的目标：
$$
\max_{\theta}\mathbb{E}_{(s_t,a_t)\sim\pi_{\theta'}}\left[\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)}A^{\theta'}(s_t,a_t)\right]
$$
最朴素的想法是添加一个约束条件，不要让 $\pi_{\theta}$ 和 $\pi_{\theta'}$ 两个分布差异太大：
$$
\begin{align}
\max_{\theta}\ &\mathbb{E}_{(s_t,a_t)\sim\pi_{\theta'}}\left[\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)}A^{\theta'}(s_t,a_t)\right]\\
\text{s.t. }\ &\mathbb{E}[\mathrm{KL}[\pi_{\theta}(\cdot\mid s_t),\pi_{\theta'}(\cdot\mid s_t)]]\leq\delta
\end{align}
$$
这个最朴素的思想便是 TRPO (Trust Region Policy Optimization) 方法了，不过带约束的问题较难求解，将硬约束转换为惩罚值，把问题变成无约束的来求解更方便：
$$
\max_{\theta}\ \mathbb{E}_{(s_t,a_t)\sim\pi_{\theta'}}\left[\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)}A^{\theta'}(s_t,a_t)-\beta\;\mathrm{KL}[\pi_{\theta}(\cdot\mid s_t),\pi_{\theta'}(\cdot\mid s_t)]\right]
$$
但这又带来一个问题，$\beta$ 这个超参数的选择较为困难。很难选择一个适应于多种问题的 $\beta$，甚至在单一的问题求解中，设定固定的 $\beta$ 也表现不佳。

# 3 PPO 方法

在 PPO 的论文中，提出了两种方法，分别是：

1. 截断代理目标 (Clipped Surrogate Objective)
2. 自适应 KL 惩罚系数 (Adaptive KL Penalty Coefficient)

## 3.1 截断代理目标

这种方法的目标是：
$$
\max_{\theta}\ \mathbb{E}_{(s_t,a_t)\sim\pi_{\theta'}}\left[\min\left(\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)}A^{\theta'}(s_t,a_t),\mathrm{clip}\left(\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)},1-\epsilon,1+\epsilon\right)A^{\theta'}(s_t,a_t)\right)\right]
$$
其中：
$$
\mathrm{clip}(x,y,z)=
\begin{align}
\begin{cases}
y&,\text{if}\;x<y\\
x&,\text{if}\;y\leq x\leq z\\
z&,\text{if}\;x>z\\
\end{cases}
\end{align}
$$
这个公式看着花里胡哨的，那就先看期望值里面的内容，画个图出来就好理解了。左边是 $A>0$ 的情况，右边是 $A<0$ 的情况：

| ![](https://assets.zouht.com/img/note/185-04.webp) | ![](https://assets.zouht.com/img/note/185-05.webp) |
| -------------------------------------------------- | -------------------------------------------------- |

对于 $A>0$ 的情况，学习时会设法通过变大 $p_{\theta}(a_t\mid s_t)$ 这个概率来最大化期望，但是变大 $p_{\theta}(a_t\mid s_t)$ 同时会导致 $\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)}$ 变大。当这个比值大于设定的 $1+\epsilon$ 后，目标函数值将会被截断，“告诉”模型再往上调整也不会获得收益了。反之，对于 $A<0$ 的情况也是一样的，就是加了个负号而已。

可以看到，这个方法的思路实际上很直白，就是通过限制模型通过加大 $\theta$ 和 $\theta'$ 的差距获得的收益，来维持两个分布相似。

## 3.2 自适应 KL 惩罚系数

这种方法实际上就是让带惩罚的 TRPO 方法的惩罚值可以自适应，它的目标和上文一模一样：
$$
\max_{\theta}\ \mathbb{E}_{(s_t,a_t)\sim\pi_{\theta'}}\left[\frac{p_{\theta}(a_t\mid s_t)}{p_{\theta'}(a_t\mid s_t)}A^{\theta'}(s_t,a_t)-\beta\;\mathrm{KL}[\pi_{\theta}(\cdot\mid s_t),\pi_{\theta'}(\cdot\mid s_t)]\right]
$$
唯一区别就是 $\beta$ 会进行自适应。首先设定一个目标差异 $d_{\text{target}}$：

- 计算 $d=\mathbb{E}[\mathrm{KL}[\pi_{\theta}(\cdot\mid s_t),\pi_{\theta'}(\cdot\mid s_t)]]$
  - 如果 $d<\frac{d_{\text{target}}}{1.5}$，则 $\beta\leftarrow\frac{\beta}{2}$
  - 如果 $d>1.5\;d_{\text{target}}$，则 $\beta\leftarrow2\beta$

其中的超参数 $1.5$ 和 $2$ 是启发式选取得到的，但这个算法实际上对这两个超参不敏感。

这个方法的思路实际上也很直白：如果我们发现 $\theta$ 和 $\theta'$ 的差距太小，这代表 $\beta$ 的限制可能太大了，模型不愿意去调整 $\theta$ 的分布，于是将 $\beta$ 改小；如果发现 $\theta$ 和 $\theta'$ 的差距太大，这代表 $\beta$ 的限制不够，于是将 $\beta$ 改大。

# 4 效果

![](https://assets.zouht.com/img/note/185-06.webp)

论文中将各种策略优化方法进行了对比实验，可以看到使用 Clip 方法的 PPO 算法在大多数场景下都优于其他算法。可以看到 PPO 算法的思路虽然朴素直白，但是效果是非常不错的。
